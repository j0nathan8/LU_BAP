---
title: "JReed Thesis"
author: "Jonathan Reed"
date: "2025-04-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = "svg")

library(readxl)     #This package is for reading Excel files
library(glmnet)     #This package is for Ridge and Lasso regression
library(caret)      #This package is for data partitioning and cross-validation
library(dplyr)      #This package is for data manipulation (used implicitly or for folds)
library(knitr)      #This package is for rendering tables (used with kable,

data1 <- read_excel("Dataset2.xlsx") #Here I am importing my dataset
data <- data1[c("yturnout",  "elec_fric", "elwi", "same", "prop", "reg", "reg_dl", 
                  "vot_age", "avg_edu", "eq_edu", "lit",  "dem")] #Here I am creating a subset of my dataset for this model

```

```{r Dataset Split}
set.seed(42) #Setting seed for reproducibility
trainIndex <- createDataPartition(data$yturnout, p = 0.85, list = FALSE) #Cutting the data with a 70/30 split into train and test data
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
```


```{r Matrices for glmnet}
x_train <- as.matrix(trainData[, -1]) #Here I am dropping the intercept in the train set
x_train <- scale(x_train) #Here I am standardising my predictor matrix
y_train <- trainData$yturnout
x_test <- as.matrix(testData[, -1]) #Here I am dropping the intercept in the test set
x_test <- scale(x_test, center = attr(x_train, "scaled:center"), scale = attr(x_train, "scaled:scale")) #Here I am standardising my predictor matrix
y_test <- testData$yturnout
x <- model.matrix(yturnout~.,data)[,-1] #Here I am creating a matrix for the IVs
y <- data$yturnout
```

```{r Cross-Validated Ridge}
gsize=100
grid=5^seq(10,-2,length=gsize) #Here I am creating a grid for the lambda values
ridge.mod=glmnet(x_train,y_train,alpha=0,lambda=grid,standardize=FALSE) #Here I am running an un-validated model across all lambda values for the graph below
cmat = coef(ridge.mod) 
cmat = cmat[2:nrow(cmat),] #Here I am dropping intercept
rgy = range(cmat)
cpar = log(1/grid)

plot(range(cpar),rgy,type='n',xlab='log(1/lambda)',ylab='Coefficients',cex.lab=1.5)
for(i in 1:nrow(cmat)) lines(cpar,cmat[i,],col=i+1,type='l') #Here I am plotting coefficients against all lambda values

cv.out = cv.glmnet(x_train,y_train,alpha=0,lambda=grid) #Here I am performing k-fold cross validation for ridge regression using the glmnet package. X is the predictor matrix, y is the response vector, alpha = 0 means a ridge regression as opposed to 1 for lasso. I did not specify cv.outr or ridge, as the variable is only used in this chunk and will be overwritten for the lasso model

cmp = log(1/cv.out$lambda) #Here I am transforming the Lambda values for plotting
plot(cmp,cv.out$cvm,type='b',xlab='cmp = log(1/lambda)',cex.lab=1.5)
bestlam = cv.out$lambda.min
bestcmp = log(1/bestlam)
text(-6, 220, paste('The ideal lambda value is: ', round(bestlam, 2)), col='red', cex=1.5)
abline(v=bestcmp,col='red') #Here I am plotting the Lambda values

ridgepred = predict(ridge.mod, s = bestlam, newx = x_test) #Here I am running the trained model on the test data, using the ideal lambda
bestridgecoef = predict(ridge.mod,s=bestlam,type='coefficients',exact=TRUE)[,1] #Here I am storing the predicted coefficients from the trained model for later

ddf = data.frame(x,y)
lm.mod = lm(y~.,ddf)
lm.fit = lm.mod$fitted #Here I am creating a linear model to compare against my model
```

```{r Cross-Validated Lasso}
lasso.mod=glmnet(x_train,y_train,alpha=1,lambda=grid,standardize=TRUE)#Here I am running an un-validated model across all lambda values for the graph below
cmat = coef(lasso.mod) 
cmat = cmat[2:nrow(cmat),] #Here I am dropping the intercept
rgy = range(cmat)
cpar = log(1/grid)

plot(range(cpar),rgy,type='n',xlab='log(1/lambda)',ylab='Coefficients',cex.lab=1.5)
for(i in 1:nrow(cmat)) lines(cpar,cmat[i,],col=i+1,type='l') #Here I am plotting coefficients against all lambda values
cvl.out = cv.glmnet(x_train,y_train,alpha=1,lambda=grid,standardize=TRUE) #Here I am performing k-fold cross validation for lasso regression using the glmnet package. X is the predictor matrix, y is the response vector, alpha = 1 means a lasso regression. 

cmp = log(1/cvl.out$lambda) #Here I am transforming the Lambda values for plotting
plot(cmp,cvl.out$cvm,type='b',xlab='cmp = log(1/lambda)',cex.lab=1.5)
bestlam = cvl.out$lambda.min
bestcmp = log(1/bestlam)
text(-5, 220, paste('The ideal lambda value is: ', round(bestlam, 2)), col='red', cex=1.5)
abline(v=bestcmp,col='red') #Here I am plotting the Lambda values

lassopred = predict(cvl.out, s = bestlam, newx = x_test) #Here I am running the trained model on the test data using the ideal lamdba
bestlassocoef = predict(cvl.out,s=bestlam,type='coefficients',exact=TRUE)[,1] #Here I am storing the predicted coefficients from the trained model for later
```


```{r Output Results}
var_names = c("Youth Voter Turnout (YVT)",  "Documents Required to Vote", "Electoral Window", "Same Day", "Proportionality",  "Registration", "Registration Deadline", "Voting Age",  "Average Education Level", "Equality of Education Access", "Literacy Rate",  "Democracy Score")
var_desc = c('The amount of voters between 18-25 who voted (%)',  "How many documents are needed on election day (1-3)", "Can people vote over multiple days (Y/N)", "Are elections held on the same day of the week every time (Y/N)", "How proportional the electoral system is (1-5)", "Whether one must register for an election (Y/N)",  "Amount of days before election day is the registration deadline (Days)", "Starting at what age can people vote (Years)", "Average Education Level Obtained (1-3)", "How equal is the distribution of education access (-3 - 3)",  "Amount of the population that can read (%)",   "How democratic is the country (0-1)")
#Above I am creating the information for the coefficient table

plot(y_test, type = "b", col = "black", pch = 16, xlab = "Youth Turnout", ylab = "Predicted Value", main = "Comparison of Predictions")
lines(ridgepred, col = "blue", lwd = 5)  #Ridge predictions in blue
lines(lassopred, col = "red", lwd = 2)   #Lasso predictions in red
legend("topright", legend = c("Actual", "Ridge", "Lasso"), col = c("black", "blue", "red"), lty = 1, lwd = 2)

ridgepred <- as.vector(ridgepred)
lassopred <- as.vector(lassopred)

#Here I am calculating the baseline RMSE
baseline_pred <- rep(mean(y_train), length(y_test))
rss <- sum((y_test - lassopred)^2)            
tss <- sum((y_test - mean(y_test))^2)    
#Here I am determining the MAE for the ridge and lasso mdoels, as well as the baseline
mae <- function(y_test, ridgepred) {
  mean(abs(y_test - ridgepred))
}

mae_value <- mae(y_test, ridgepred)
print(paste("Ridge MAE:", mae_value))
mae <- function(y_test, lassopred) {
  mean(abs(y_test - lassopred))
}

mae_value <- mae(y_test, lassopred)
print(paste("Lasso MAE:", mae_value))
basline_pred <- mean(y_test)
baseline_preds <- rep(baseline_pred, length(y_test))
baseline_mae <- mean(abs(y_test - baseline_preds))
print(paste("Baseline MAE:", baseline_mae))

#Below is my code for determining the RMSEs of the model
ridge_rmse <- sqrt(mean((y_test - ridgepred)^2))
lasso_rmse <- sqrt(mean((y_test - lassopred)^2))
baseline_rmse <- sqrt(mean((y_test - baseline_pred)^2))
print(paste("Ridge Model RMSE:", round(ridge_rmse, 2)))
print(paste("Lasso Model RMSE:", round(lasso_rmse, 2)))
print(paste("Baseline RMSE (Mean Prediction):", round(baseline_rmse, 2)))

intercept_ridge <- bestridgecoef[1]
intercept_lasso <- bestlassocoef[1]

#Here I am calculating the percentage change for each coefficient (excluding the intercept)
ridge_pct_change <- round(((bestridgecoef[-1] / intercept_ridge) * 100), 2)
lasso_pct_change <- round(((bestlassocoef[-1] / intercept_lasso) * 100), 2)

ridge_pct_change <- c(0, ridge_pct_change)
lasso_pct_change <- c(0, lasso_pct_change)

#Here is my code for outputting the coefficients table 
ridge_combined <- paste0(round(bestridgecoef, 2), "<br>(", ridge_pct_change, "%)")
lasso_combined <- paste0(round(bestlassocoef, 2), "<br>(", lasso_pct_change, "%)")

coef_table <- data.frame(
  Variable = var_names,
  Variable.Description = var_desc,
  Ridge = ridge_combined,
  Lasso = lasso_combined,
  stringsAsFactors = FALSE
)


write.csv(coef_table, "coef_table.csv", row.names = FALSE)

knitr::kable(coef_table, format = "html", escape = FALSE, caption = "Ridge and Lasso Coefficient Comparison")

```